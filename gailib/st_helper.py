import logging
import random
import re
import time
from collections import OrderedDict
from datetime import datetime, timedelta
from typing import List

import azure.cognitiveservices.speech as speechsdk
import pytz
import streamlit as st
import streamlit.components.v1 as components
import vertexai
from azure.cognitiveservices.speech import (
    ResultReason,
    SpeechSynthesisCancellationDetails,
)

# from annotated_text import annotated_text, annotation
from azure.storage.blob import BlobServiceClient
from google.cloud import firestore, translate
from google.oauth2.service_account import Credentials
from vertexai.preview.generative_models import GenerativeModel, Image

from .azure_pronunciation_assessment import (
    get_syllable_durations_and_offsets,
    pronunciation_assessment_from_stream,
)
from .azure_speech import synthesize_speech
from .constants import USD_TO_CNY_EXCHANGE_RATE
from .db_interface import DbInterface
from .google_ai import MAX_CALLS, PER_SECONDS, ModelRateLimiter
from .google_cloud_configuration import (
    LOCATION,
    PROJECT_ID,
    get_google_service_account_info,
    google_configure,
)
from .html_constants import TIPPY_JS
from .html_fmt import pronunciation_assessment_word_format
from .utils import calculate_audio_duration
from .word_utils import (
    audio_autoplay_elem,
    get_mini_dict,
    get_word_image_urls,
    load_image_bytes_from_url,
)

# å•ä¸ªå•è¯å•æ¬¡æœ€é•¿å­¦ä¹ æ—¶é•¿
MAX_WORD_STUDY_TIME = 60  # 60ç§’
ABNORMAL_DURATION = 60 * 60  # 1å°æ—¶
DB_TIME_INTERVAL = 3 * 60  # 3 åˆ†é’Ÿ
logger = logging.getLogger("streamlit")

# å‘éŸ³è¯„ä¼°(éŸµå¾‹ã€è¯­æ³•ã€è¯æ±‡ã€ä¸»é¢˜)
RATE_PER_HOUR = 0.3
# å®æ—¶å’Œæ‰¹å¤„ç†åˆæˆ: $15/æ¯ 100 ä¸‡ å­—ç¬¦
# é•¿éŸ³é¢‘åˆ¶ä½œï¼š æ¯ 100 ä¸‡ä¸ªå­—ç¬¦ $100
MIN_RATE_PER_MILLION_CHARS = 15
MAX_RATE_PER_MILLION_CHARS = 100
# Google ç¿»è¯‘è´¹ç‡ per million characters
RATE_PER_MILLION_CHARS = 25

TOEKN_HELP_INFO = (
    "âœ¨ å¯¹äº Gemini æ¨¡å‹ï¼Œä¸€ä¸ªä»¤ç‰Œçº¦ç›¸å½“äº 4 ä¸ªå­—ç¬¦ã€‚100 ä¸ªè¯å…ƒçº¦ä¸º 60-80 ä¸ªè‹±è¯­å•è¯ã€‚"
)

# region é€šç”¨å‡½æ•°


def setup_logger(logger, level="INFO"):
    # è®¾ç½®æ—¥å¿—çš„æ—¶é—´æˆ³ä¸º Asia/Shanghai æ—¶åŒº
    formatter = logging.Formatter("%(asctime)s - %(message)s")
    formatter.converter = lambda *args: datetime.now(
        tz=pytz.timezone("Asia/Shanghai")
    ).timetuple()
    for handler in logger.handlers:
        handler.setFormatter(formatter)
        handler.setLevel(logging.getLevelName(level))


setup_logger(logger)


def count_non_none(lst):
    return len(list(filter(lambda x: x is not None, lst)))


def is_answer_correct(user_answer, standard_answer):
    # å¦‚æœç”¨æˆ·æ²¡æœ‰é€‰æ‹©ç­”æ¡ˆï¼Œç›´æ¥è¿”å› False
    if user_answer is None:
        return False

    # åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼Œå°†é€‰é¡¹åºå·æ˜ å°„åˆ°å­—æ¯
    answer_dict = {0: "A", 1: "B", 2: "C", 3: "D"}

    # æ£€æŸ¥ç”¨æˆ·ç­”æ¡ˆæ˜¯å¦æ˜¯ä¸€ä¸ªæ•´æ•°
    if isinstance(user_answer, int):
        # è·å–ç”¨æˆ·çš„ç­”æ¡ˆå¯¹åº”çš„å­—æ¯
        user_answer = answer_dict.get(user_answer, "")
    else:
        # ç§»é™¤ç”¨æˆ·ç­”æ¡ˆä¸­çš„éå­—æ¯å­—ç¬¦ï¼Œå¹¶åªå–ç¬¬ä¸€ä¸ªå­—ç¬¦
        user_answer = "".join(filter(str.isalpha, user_answer))[0]

    # ç§»é™¤æ ‡å‡†ç­”æ¡ˆä¸­çš„éå­—æ¯å­—ç¬¦ï¼Œå¹¶åªå–ç¬¬ä¸€ä¸ªå­—ç¬¦
    standard_answer = "".join(filter(str.isalpha, standard_answer))[0]

    # æ¯”è¾ƒç”¨æˆ·çš„ç­”æ¡ˆå’Œæ ‡å‡†ç­”æ¡ˆ
    return user_answer == standard_answer


# endregion


# region ç”¨æˆ·ç›¸å…³


def check_access(is_admin_page):
    if not st.session_state.dbi.is_logged_in():
        st.error("æ‚¨å°šæœªç™»å½•ã€‚è¯·ç‚¹å‡»å±å¹•å·¦ä¾§ ğŸ  ä¸»é¡µ èœå•è¿›è¡Œç™»å½•ã€‚")
        st.stop()

    if (
        is_admin_page
        and st.session_state.dbi.cache.get("user_info", {}).get("user_role") != "ç®¡ç†å‘˜"
    ):
        st.error("æ‚¨æ²¡æœ‰æƒé™è®¿é—®æ­¤é¡µé¢ã€‚æ­¤é¡µé¢ä»…ä¾›ç³»ç»Ÿç®¡ç†å‘˜ä½¿ç”¨ã€‚")
        st.stop()


# endregion

# region Google


@st.cache_resource
def get_translation_client():
    service_account_info = get_google_service_account_info(st.secrets)
    # åˆ›å»ºå‡­æ®
    credentials = Credentials.from_service_account_info(service_account_info)
    # ä½¿ç”¨å‡­æ®åˆå§‹åŒ–å®¢æˆ·ç«¯
    return translate.TranslationServiceClient(credentials=credentials)


@st.cache_resource
def get_firestore_client():
    service_account_info = get_google_service_account_info(st.secrets)
    # åˆ›å»ºå‡­æ®
    credentials = Credentials.from_service_account_info(service_account_info)
    # ä½¿ç”¨å‡­æ®åˆå§‹åŒ–å®¢æˆ·ç«¯
    return firestore.Client(credentials=credentials, project=PROJECT_ID)


def configure_google_apis():
    # é…ç½® AI æœåŠ¡
    if st.secrets["env"] in ["streamlit", "azure"]:
        if "inited_google_ai" not in st.session_state:
            google_configure(st.secrets)
            # vertexai.init(project=PROJECT_ID, location=LOCATION)
            st.session_state["inited_google_ai"] = True

        if "rate_limiter" not in st.session_state:
            st.session_state.rate_limiter = ModelRateLimiter(MAX_CALLS, PER_SECONDS)

        if "google_translate_client" not in st.session_state:
            st.session_state["google_translate_client"] = get_translation_client()

        # é…ç½® token è®¡æ•°å™¨
        if "current_token_count" not in st.session_state:
            st.session_state["current_token_count"] = 0

        if "total_token_count" not in st.session_state:
            st.session_state["total_token_count"] = (
                st.session_state.dbi.get_token_count()
            )
    else:
        st.warning("éäº‘ç«¯ç¯å¢ƒï¼Œæ— æ³•ä½¿ç”¨ Google AI", icon="âš ï¸")


def google_translate(
    item_name, text, target_language_code: str = "zh-CN", is_list: bool = False
):
    """Translating Text."""
    # Cloud Translation ä¼šæŒ‰å­—ç¬¦æ•°ç»Ÿè®¡ç”¨é‡ï¼Œå³ä½¿ä¸€ä¸ªå­—ç¬¦ä¸ºå¤šå­—èŠ‚ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ç©ºç™½å­—ç¬¦ä¹Ÿéœ€è¦ä»˜è´¹ã€‚
    # LLM $25 per million characters $20 per million characters
    if is_list:
        if not isinstance(text, list):
            raise ValueError("Expected a list of strings, but got a single string.")
        if not all(isinstance(i, str) for i in text):
            raise ValueError("All elements in the list should be strings.")
    else:
        if not isinstance(text, str):
            raise ValueError("Expected a string, but got a different type.")

    if not text or text == "":
        return text  # type: ignore

    # è®¡ç®—å­—ç¬¦æ•°
    char_count = len("".join(text)) if is_list else len(text)
    # è®¡ç®—è´¹ç”¨
    cost = (char_count / 1000000) * RATE_PER_MILLION_CHARS * USD_TO_CNY_EXCHANGE_RATE

    # Location must be 'us-central1' or 'global'.
    parent = f"projects/{PROJECT_ID}/locations/global"

    client = st.session_state.google_translate_client
    # Detail on supported types can be found here:
    # https://cloud.google.com/translate/docs/supported-formats
    response = client.translate_text(
        request={
            "parent": parent,
            "contents": text if is_list else [text],
            "mime_type": "text/plain",  # mime types: text/plain, text/html
            "source_language_code": "en-US",
            "target_language_code": target_language_code,
        }
    )

    res = []
    # Display the translation for each input text provided
    for translation in response.translations:
        res.append(translation.translated_text.encode("utf8").decode("utf8"))
    # google translate api è¿”å›ä¸€ä¸ªç»“æœ
    usage = {
        "service_name": "Google ç¿»è¯‘",
        "char_count": char_count,
        "cost": cost,
        "item_name": item_name,
        "timestamp": datetime.now(pytz.UTC),
    }
    st.session_state.dbi.add_usage_to_cache(usage)
    # logger.info(f"ç¿»è¯‘è´¹ç”¨ï¼š{cost:.4f}å…ƒï¼Œå­—ç¬¦æ•°ï¼š{char_count}")
    return res if is_list else res[0]


@st.cache_data(ttl=timedelta(days=1))  # ç¼“å­˜æœ‰æ•ˆæœŸä¸º24å°æ—¶
def translate_text(item_name, text: str, target_language_code, is_list: bool = False):
    return google_translate(item_name, text, target_language_code, is_list)


# endregion


# region Azure
@st.cache_resource
def get_blob_service_client():
    # container_name = "word-images"
    connect_str = st.secrets["Microsoft"]["AZURE_STORAGE_CONNECTION_STRING"]
    # åˆ›å»º BlobServiceClient å¯¹è±¡
    return BlobServiceClient.from_connection_string(connect_str)


@st.cache_resource
def get_blob_container_client(container_name):
    # åˆ›å»º BlobServiceClient å¯¹è±¡
    blob_service_client = get_blob_service_client()
    # è·å– ContainerClient å¯¹è±¡
    return blob_service_client.get_container_client(container_name)


# endregion

# region æ’­æ”¾æ˜¾ç¤º


def format_token_count(count):
    if count >= 1000000000:
        return f"{count / 1000000000:.2f}B"
    elif count >= 1000000:
        return f"{count / 1000000:.2f}M"
    elif count >= 1000:
        return f"{count / 1000:.2f}K"
    else:
        return str(count)


def update_and_display_progress(
    current_value: int, total_value: int, progress_bar, message=""
):
    """
    æ›´æ–°å¹¶æ˜¾ç¤ºè¿›åº¦æ¡ã€‚

    Args:
        current_value (int): å½“å‰å€¼ã€‚
        total_value (int): æ€»å€¼ã€‚
        progress_bar: Streamlit progress bar object.

    Returns:
        None
    """
    # è®¡ç®—è¿›åº¦
    progress = current_value / total_value

    # æ˜¾ç¤ºè¿›åº¦ç™¾åˆ†æ¯”
    text = f"{progress:.2%} {message}"

    # æ›´æ–°è¿›åº¦æ¡çš„å€¼
    progress_bar.progress(progress, text)


def view_md_badges(
    container, d: dict, badge_maps: OrderedDict, decimal_places: int = 2
):
    cols = container.columns(len(badge_maps.keys()))
    for i, t in enumerate(badge_maps.keys()):
        n = d.get(t, None)
        if n is None:
            num = "0"
        elif isinstance(n, int):
            num = f"{n:3d}"
        elif isinstance(n, float):
            num = f"{n:.{decimal_places}f}"
        else:
            num = n
        body = f"""{badge_maps[t][1]}[{num}]"""
        cols[i].markdown(
            f""":{badge_maps[t][0]}[{body}]""",
            help=f"âœ¨ {badge_maps[t][2]}",
        )


def autoplay_audio_and_display_text(
    elem, audio_bytes: bytes, words: List[speechsdk.PronunciationAssessmentWordResult]
):
    """
    è‡ªåŠ¨æ’­æ”¾éŸ³é¢‘å¹¶æ˜¾ç¤ºæ–‡æœ¬ã€‚

    Args:
        elem: æ˜¾ç¤ºæ–‡æœ¬çš„å…ƒç´ ã€‚
        audio_bytes: éŸ³é¢‘æ–‡ä»¶çš„å­—èŠ‚æ•°æ®ã€‚
        words: åŒ…å«å‘éŸ³è¯„ä¼°å•è¯ç»“æœçš„åˆ—è¡¨ã€‚

    Returns:
        None
    """
    auto_html = audio_autoplay_elem(audio_bytes, fmt="wav")
    components.html(auto_html)

    start_time = time.perf_counter()
    for i, (accumulated_text, duration, offset, _) in enumerate(
        get_syllable_durations_and_offsets(words)
    ):
        elem.markdown(accumulated_text + "â–Œ")
        time.sleep(duration)
        while time.perf_counter() - start_time < offset:
            time.sleep(0.01)
    elem.markdown(accumulated_text)
    # time.sleep(1)
    # st.rerun()


def update_sidebar_status(sidebar_status):
    sidebar_status.markdown(
        f"""ä»¤ç‰Œï¼š{st.session_state.current_token_count} ç´¯è®¡ï¼š{format_token_count(st.session_state.total_token_count)}""",
        help=TOEKN_HELP_INFO,
    )


# endregion

# region å•è¯ä¸å‘éŸ³è¯„ä¼°

WORD_COUNT_BADGE_MAPS = OrderedDict(
    {
        "å•è¯æ€»é‡": ("green", "å•è¯æ€»é‡", "æ–‡æœ¬ä¸­ä¸é‡å¤çš„å•è¯æ•°é‡", "success"),
        "A1": ("orange", "A1", "CEFR A1 å•è¯æ•°é‡", "warning"),
        "A2": ("grey", "A2", "CEFR A1 å•è¯æ•°é‡", "secondary"),
        "B1": ("red", "B1", "CEFR B1 å•è¯æ•°é‡", "danger"),
        "B2": ("violet", "B2", "CEFR B2 å•è¯æ•°é‡", "info"),
        "C1": ("blue", "C1", "CEFR C1 å•è¯æ•°é‡", "light"),
        "C2": ("rainbow", "C2", "CEFR C2 å•è¯æ•°é‡", "dark"),
        "æœªåˆ†çº§": ("green", "æœªåˆ†çº§", "æœªåˆ†çº§å•è¯æ•°é‡", "dark"),
    }
)

PRONUNCIATION_SCORE_BADGE_MAPS = OrderedDict(
    {
        "pronunciation_score": (
            "green",
            "ç»¼åˆè¯„åˆ†",
            "è¡¨ç¤ºç»™å®šè¯­éŸ³å‘éŸ³è´¨é‡çš„æ€»ä½“åˆ†æ•°ã€‚è¿™æ˜¯ç”± AccuracyScoreã€FluencyScoreã€CompletenessScore (å¦‚æœé€‚ç”¨)ã€ProsodyScore (å¦‚æœé€‚ç”¨)åŠ æƒèšåˆè€Œæˆã€‚",
            "success",
        ),
        "accuracy_score": (
            "orange",
            "å‡†ç¡®æ€§è¯„åˆ†",
            "è¯­éŸ³çš„å‘éŸ³å‡†ç¡®æ€§ã€‚å‡†ç¡®æ€§è¡¨ç¤ºéŸ³ç´ ä¸æ¯è¯­è¯´è¯äººçš„å‘éŸ³çš„åŒ¹é…ç¨‹åº¦ã€‚å­—è¯å’Œå…¨æ–‡çš„å‡†ç¡®æ€§å¾—åˆ†æ˜¯ç”±éŸ³ç´ çº§çš„å‡†ç¡®åº¦å¾—åˆ†æ±‡æ€»è€Œæ¥ã€‚",
            "warning",
        ),
        "fluency_score": (
            "grey",
            "æµç•…æ€§è¯„åˆ†",
            "ç»™å®šè¯­éŸ³çš„æµç•…æ€§ã€‚æµç•…æ€§è¡¨ç¤ºè¯­éŸ³ä¸æ¯è¯­è¯´è¯äººåœ¨å•è¯é—´çš„åœé¡¿ä¸Šæœ‰å¤šæ¥è¿‘ã€‚",
            "secondary",
        ),
        "completeness_score": (
            "red",
            "å®Œæ•´æ€§è¯„åˆ†",
            "è¯­éŸ³çš„å®Œæ•´æ€§ï¼ŒæŒ‰å‘éŸ³å•è¯ä¸è¾“å…¥å¼•ç”¨æ–‡æœ¬çš„æ¯”ç‡è®¡ç®—ã€‚",
            "danger",
        ),
        "prosody_score": (
            "rainbow",
            "éŸµå¾‹è¯„åˆ†",
            "ç»™å®šè¯­éŸ³çš„éŸµå¾‹ã€‚éŸµå¾‹æŒ‡ç¤ºç»™å®šè¯­éŸ³çš„æ€§è´¨ï¼ŒåŒ…æ‹¬é‡éŸ³ã€è¯­è°ƒã€è¯­é€Ÿå’ŒèŠ‚å¥ã€‚",
            "info",
        ),
    }
)

ORAL_ABILITY_SCORE_BADGE_MAPS = OrderedDict(
    {
        "content_score": (
            "green",
            "å£è¯­èƒ½åŠ›",
            "è¡¨ç¤ºå­¦ç”Ÿå£è¯­èƒ½åŠ›çš„æ€»ä½“åˆ†æ•°ã€‚ç”±è¯æ±‡å¾—åˆ†ã€è¯­æ³•å¾—åˆ†å’Œä¸»é¢˜å¾—åˆ†çš„ç®€å•å¹³å‡å¾—å‡ºã€‚"
            "success",
        ),
        "vocabulary_score": (
            "rainbow",
            "è¯æ±‡åˆ†æ•°",
            "è¯æ±‡è¿ç”¨èƒ½åŠ›çš„ç†Ÿç»ƒç¨‹åº¦æ˜¯é€šè¿‡è¯´è¯è€…æœ‰æ•ˆåœ°ä½¿ç”¨å•è¯æ¥è¯„ä¼°çš„ï¼Œå³åœ¨ç‰¹å®šè¯­å¢ƒä¸­ä½¿ç”¨æŸå•è¯ä»¥è¡¨è¾¾è§‚ç‚¹æ˜¯å¦æ°å½“ã€‚",
            "warning",
        ),
        "grammar_score": (
            "blue",
            "è¯­æ³•åˆ†æ•°",
            "æ­£ç¡®ä½¿ç”¨è¯­æ³•çš„ç†Ÿç»ƒç¨‹åº¦ã€‚è¯­æ³•é”™è¯¯æ˜¯é€šè¿‡å°†é€‚å½“çš„è¯­æ³•ä½¿ç”¨æ°´å¹³ä¸è¯æ±‡ç»“åˆè¿›è¡Œè¯„ä¼°çš„ã€‚",
            "secondary",
        ),
        "topic_score": (
            "orange",
            "ä¸»é¢˜åˆ†æ•°",
            "å¯¹ä¸»é¢˜çš„ç†è§£å’Œå‚ä¸ç¨‹åº¦ï¼Œå®ƒæä¾›æœ‰å…³è¯´è¯äººæœ‰æ•ˆè¡¨è¾¾å…¶æ€è€ƒå’Œæƒ³æ³•çš„èƒ½åŠ›ä»¥åŠå‚ä¸ä¸»é¢˜çš„èƒ½åŠ›çš„è§è§£ã€‚",
            "danger",
        ),
    }
)


# åˆ¤æ–­æ˜¯å¦ä¸ºæ—ç™½
def is_aside(text):
    return re.match(r"^\(.*\)$", text) is not None


@st.cache_data(max_entries=10000, ttl=timedelta(days=1), show_spinner=False)
def get_synthesis_speech(text, voice):
    # é¦–å…ˆå¤„ç†textï¼Œåˆ é™¤textä¸­çš„ç©ºç™½è¡Œ
    text = re.sub("\n\\s*\n*", "\n", text)
    is_free = True
    try:
        result = synthesize_speech(
            text,
            st.secrets["Microsoft"]["F0_SPEECH_KEY"],
            st.secrets["Microsoft"]["F0_SPEECH_REGION"],
            voice,
        )
        if result.reason == ResultReason.Canceled:
            cancellation_details = SpeechSynthesisCancellationDetails(result)
            logger.error(f"Speech synthesis canceled: {cancellation_details.reason}")
            logger.error(f"Error details: {cancellation_details.error_details}")
    except Exception as e:
        is_free = False
        result = synthesize_speech(
            text,
            st.secrets["Microsoft"]["SPEECH_KEY"],
            st.secrets["Microsoft"]["SPEECH_REGION"],
            voice,
        )

    if is_free:
        cost0 = 0.0
        cost1 = 0.0
    else:
        cost0 = (
            (len(text) / 1000000)
            * MIN_RATE_PER_MILLION_CHARS
            * USD_TO_CNY_EXCHANGE_RATE
        )
        cost1 = (
            (len(text) / 1000000)
            * MAX_RATE_PER_MILLION_CHARS
            * USD_TO_CNY_EXCHANGE_RATE
        )

    # å®æ—¶å’Œæ‰¹å¤„ç†åˆæˆ: $15/æ¯ 100 ä¸‡ å­—ç¬¦
    # é•¿éŸ³é¢‘åˆ¶ä½œï¼š æ¯ 100 ä¸‡ä¸ªå­—ç¬¦ $100
    char_count = len(text)
    cost = (
        (char_count / 1000000) * MAX_RATE_PER_MILLION_CHARS * USD_TO_CNY_EXCHANGE_RATE
    )
    usage = {
        "service_name": "å¾®è½¯è¯­éŸ³æœåŠ¡",
        "char_count": char_count,
        "cost": cost,
        "cost0": cost0,
        "cost1": cost1,
        "item_name": "è¯­éŸ³åˆæˆ",
        "timestamp": datetime.now(pytz.UTC),
    }
    st.session_state.dbi.add_usage_to_cache(usage)
    # logger.info(f"è¯­éŸ³åˆæˆè´¹ç”¨ï¼š{cost:.4f}å…ƒï¼Œå­—ç¬¦æ•°ï¼š{char_count}")
    # free_flag = "å…è´¹" if is_free else "ä»˜è´¹"
    # logger.info(
    #     f"è¯­éŸ³åˆæˆè´¹ç”¨ï¼š{cost0:.4f}å…ƒï¼Œå­—ç¬¦æ•°ï¼š{char_count}ï¼Œæ˜¯å¦å…è´¹ï¼š{free_flag}ï¼Œè´¹ç”¨1ï¼š{cost1:.4f}å…ƒ"
    # )
    return {"audio_data": result.audio_data, "audio_duration": result.audio_duration}


@st.cache_resource
def load_mini_dict():
    return get_mini_dict()


@st.cache_resource(
    show_spinner="æå–ç®€ç‰ˆè¯å…¸å•è¯ä¿¡æ¯...", ttl=timedelta(days=1)
)  # ç¼“å­˜æœ‰æ•ˆæœŸä¸º24å°æ—¶
def get_mini_dict_doc(word):
    w = word.replace("/", " or ")
    mini_dict = load_mini_dict()
    return mini_dict.get(w, {})


@st.cache_data(
    ttl=timedelta(days=1), max_entries=10000, show_spinner="è·å–å•è¯å›¾ç‰‡ç½‘å€..."
)
def select_word_image_urls(word: str):
    mini_dict_doc = get_mini_dict_doc(word)
    return mini_dict_doc.get("image_urls", [])


def pronunciation_assessment_with_cost(
    audio_info: dict, topic: str, reference_text: str
):
    # $0.30 /å°æ—¶/åŠŸèƒ½
    duration = calculate_audio_duration(
        audio_info["bytes"], audio_info["sample_rate"], audio_info["sample_width"]
    )
    cost = (duration / 3600) * RATE_PER_HOUR * USD_TO_CNY_EXCHANGE_RATE
    is_oral = topic is not None
    usage = {
        "service_name": "å¾®è½¯è¯­éŸ³æœåŠ¡",
        "item_name": "å£è¯­èƒ½åŠ›è¯„ä¼°" if is_oral else "å‘éŸ³è¯„ä¼°",
        "duration": duration,
        "cost": cost,
        "timestamp": datetime.now(pytz.UTC),
    }
    st.session_state.dbi.add_usage_to_cache(usage)
    # logger.info(f"å‘éŸ³è¯„ä¼°è´¹ç”¨ï¼š{cost:.4f}å…ƒï¼Œæ—¶é•¿ï¼š{duration:.2f}ç§’")
    return pronunciation_assessment_from_stream(
        audio_info, st.secrets, topic, reference_text
    )


@st.cache_data(ttl=timedelta(days=1), show_spinner="æ­£åœ¨è¿›è¡Œå‘éŸ³è¯„ä¼°ï¼Œè¯·ç¨å€™...")
def pronunciation_assessment_for(audio_info: dict, reference_text: str):
    return pronunciation_assessment_with_cost(audio_info, None, reference_text)


@st.cache_data(ttl=timedelta(days=1), show_spinner="æ­£åœ¨è¿›è¡Œå£è¯­èƒ½åŠ›è¯„ä¼°ï¼Œè¯·ç¨å€™...")
def oral_ability_assessment_for(audio_info: dict, topic: str):
    return pronunciation_assessment_with_cost(audio_info, topic, None)


def display_assessment_score(
    container, maps, assessment_key, score_key="pronunciation_result", idx=None
):
    """
    Display the assessment score for a given assessment key.

    Parameters:
    container (object): The container object to display the score.
    maps (dict): A dictionary containing mappings for the score.
    assessment_key (str): The key to retrieve the assessment from st.session_state.
    score_key (str, optional): The key to retrieve the score from the assessment. Defaults to "pronunciation_result".
    """
    if assessment_key not in st.session_state:
        return
    d = st.session_state[assessment_key]
    if idx is not None:
        result = d.get(idx, {}).get(score_key, {})
    else:
        result = d.get(score_key, {})
    if not result:
        return
    view_md_badges(container, result, maps, 0)


def process_dialogue_text(reference_text):
    # å»æ‰åŠ é»‘ç­‰æ ‡æ³¨
    reference_text = reference_text.replace("**", "")
    # å»æ‰å¯¹è¯è€…åå­—
    reference_text = re.sub(r"^\w+(\s\w+)*:\s", "", reference_text, flags=re.MULTILINE)
    # å»æ‰ç©ºè¡Œ
    reference_text = re.sub("\n\\s*\n*", "\n", reference_text)
    return reference_text.strip()


def view_word_assessment(words):
    result = ""
    for word in words:
        result += pronunciation_assessment_word_format(word)
    st.markdown(result + TIPPY_JS, unsafe_allow_html=True)


def _word_to_text(word):
    error_type = word.error_type
    accuracy_score = round(word.accuracy_score)
    if error_type == "Mispronunciation":
        return f"{word.word} | {accuracy_score}"
    if error_type == "Omission":
        return f"[{word.word}]"
    if error_type == "Insertion":
        return f"{word.word}"
    if word.is_unexpected_break:
        return f"{word.word} | {accuracy_score}"
    if word.is_missing_break:
        return f"{word.word} | {accuracy_score}"
    if word.is_monotone:
        return f"{word.word} | {accuracy_score}"
    return f"{word.word}"


# TODO:åºŸå¼ƒæˆ–ä½¿ç”¨ å•è¯æ•°é‡è°ƒæ•´
def left_paragraph_aligned_text(text1, words):
    """
    å°†æ–‡æœ¬1çš„æ¯ä¸ªæ®µè½é¦–è¡Œä¸wordsé¦–è¡Œå¯¹é½ï¼ˆä¸ºæ–‡æœ¬1è¡¥é½ç©ºè¡Œï¼‰ã€‚

    Args:
        text1 (str): åŸå§‹æ–‡æœ¬ã€‚
        words (list): è¦æ’å…¥æ–‡æœ¬ä¸­çš„å•è¯åˆ—è¡¨ã€‚

    Returns:
        str: å¤„ç†åçš„æ–‡æœ¬ã€‚
    """

    # å°†æ–‡æœ¬1åˆ†å‰²æˆæ®µè½
    paragraphs1 = text1.split("\n\n")

    if len(words) == 0:
        return paragraphs1

    # å¤„ç†å•è¯
    res = []
    for word in words:
        if isinstance(word, str):
            res.append(word)
        else:
            res.append(_word_to_text(word))
        res.append(" ")
    text2 = "".join(res)

    # å°†æ–‡æœ¬2åˆ†å‰²æˆæ®µè½
    paragraphs2 = text2.split("\n\n")

    # è®¡ç®—æ¯ä¸ªæ®µè½çš„è¡Œæ•°
    lines1 = [len(p.split("\n")) for p in paragraphs1]
    lines2 = [len(p.split("\n")) for p in paragraphs2]

    # æ·»åŠ ç©ºç™½è¡Œ
    for i in range(min(len(lines1), len(lines2))):
        diff = lines2[i] - lines1[i]
        if diff > 0:
            paragraphs1[i] += "\n" * diff

    return paragraphs1


# endregion


# region ä¸ªäººè®°å½•


def on_project_changed(project_name):
    if not st.session_state.get("role"):
        return

    if "project-timer" not in st.session_state:
        st.session_state["project-timer"] = {}

    if "current-project" in st.session_state:
        # ç»“æŸä¸Šä¸€ä¸ªé¡¹ç›®ï¼Œè®¡ç®—æ€»æ—¶é•¿
        previous_project = st.session_state["current-project"]
        if previous_project in st.session_state["project-timer"]:
            start_time = st.session_state["project-timer"][previous_project][
                "start_time"
            ]
            duration = st.session_state["project-timer"][previous_project]["duration"]
            t = time.time() - start_time
            if previous_project.startswith("å•è¯ç»ƒä¹ ") and t > MAX_WORD_STUDY_TIME:
                t = MAX_WORD_STUDY_TIME
            duration += t

            st.session_state["project-timer"][previous_project] = {
                "start_time": None,
                "end_time": time.time(),
                "duration": duration,
            }

    # å¦‚æœé¡¹ç›®å·²ç»å­˜åœ¨ï¼Œé‚£ä¹ˆç´¯è®¡ä¹‹å‰çš„æ—¶é•¿ï¼Œå¦åˆ™åˆå§‹åŒ–æ—¶é•¿ä¸º0
    if project_name in st.session_state["project-timer"]:
        previous_duration = st.session_state["project-timer"][project_name]["duration"]
    else:
        previous_duration = 0.0

    # å¼€å§‹æ–°çš„é¡¹ç›®ï¼Œè®°å½•å¼€å§‹æ—¶é—´
    st.session_state["project-timer"][project_name] = {
        "start_time": time.time(),
        "end_time": None,
        "duration": previous_duration,
    }
    st.session_state["current-project"] = project_name

    # for project, data in st.session_state["project-timer"].items():
    #     if "duration" in data:
    #         duration_seconds = data["duration"]
    #         logger.info(f"é¡¹ç›® {project} çš„æ€»æ—¶é•¿ï¼ˆç§’ï¼‰: {duration_seconds}")
    # logger.info("=====================================")


def add_exercises_to_db(force=False):
    """
    å°†ç»ƒä¹ æ•°æ®æ·»åŠ åˆ°æ•°æ®åº“ä¸­ã€‚

    å‚æ•°ï¼š
    - forceï¼šboolï¼Œå¯é€‰ï¼Œæ˜¯å¦å¼ºåˆ¶æ·»åŠ æ•°æ®åˆ°æ•°æ®åº“ã€‚é»˜è®¤ä¸ºFalseã€‚

    è¿”å›ï¼š
    æ— è¿”å›å€¼ã€‚
    """

    if "dbi" not in st.session_state:
        return

    session_id = st.session_state.dbi.cache["user_info"].get("session_id")

    if session_id is None:
        return

    if "last_commit_time" not in st.session_state:
        st.session_state["last_commit_time"] = time.time()

    if force or time.time() - st.session_state["last_commit_time"] > DB_TIME_INTERVAL:
        # é”å®šå¯¹è±¡ï¼Œé˜²æ­¢æ›´æ”¹
        project_timer = st.session_state["project-timer"].copy()
        docs = []
        for project_name, project_data in project_timer.items():
            # å¦‚åªæœ‰å¼€å§‹æ—¶é—´ï¼Œåˆ™ä»¥å½“å‰æ—¶é—´è®¡ç®—æ—¶é•¿
            if (
                project_data.get("start_time") is not None
                and project_data.get("end_time") is None
            ):
                project_data["end_time"] = time.time()
                project_data["duration"] = (
                    project_data["end_time"] - project_data["start_time"]
                )

            if project_data["duration"] <= ABNORMAL_DURATION:
                docs.append(
                    {
                        "item": project_name,
                        "duration": project_data["duration"],
                        "timestamp": datetime.now(pytz.UTC),
                    }
                )

        # ä¿å­˜æ•°æ®åˆ° Firestore
        st.session_state.dbi.add_documents_to_user_history("exercises", docs)

        # logger.info(f"ä¿å­˜æ•°æ®ï¼š{docs}")

        # æ›´æ–°æœ€åæäº¤æ—¶é—´
        st.session_state["last_commit_time"] = time.time()

        # æ¸…é™¤ä¼šè¯å†…å®¹
        st.session_state["project-timer"] = {}


# endregion
